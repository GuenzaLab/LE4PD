#!/bin/bash
#!/bin/bash -l
#PBS -N NAMEOFPROT
#PBS -q longgen
#PBS -l nodes=1:ppn=4
#PBS -d /path/to/NAMEOFPROT/
#PBS -o /path/to/NAMEOFPROT/NAMEOFPROT.log
#PBS -e /path/to/NAMEOFPROT/NAMEOFPROT.log
#PBS -j oe

protname="NAMEOFPROT"
cd /path/to/NAMEOFPROT/

module unload gromacs/4.6.1p
module load gromacs/4.5.5_sngl

temp=298
fd20=0.0
traj=~/protein/${protname}/${protname}_pro1.xtc
top=~/protein/${protname}/${protname}_pro1.tpr
SIMB=1000
SIME=2000

### this will submit to the "generic" queue on ACISS
### requesting 1 node and 1 processor per node
### change ppn to 12 if you want to run 12 processes with this script

# Load any modules needed to run your software
# the examples below load modules for running OpenMPI jobs


export NPROCS=`wc -l $PBS_NODEFILE |gawk '//{print $1}'`
echo $NPROCS

# the following lines are not required, but can be useful
# for debugging purposes:
#diplays PBS work directory
echo "PBS_O_WORKDIR:" $PBS_O_WORKDIR
#cd $PBS_O_WORKDIR

#displays nodefile and contents of nodefile, useful for running MPI
echo `rm hostfile.tmp`
echo "PBS_NODEFILE:" $PBS_NODEFILE
cat $PBS_NODEFILE > hostfile.tmp

#displays PBS jobname and jobid
echo "PBS_JOBNAME, PBS_JOBID:" $PBS_JOBNAME $PBS_JOBID

#displays username and hostname,
export USER_NAME=`whoami`
export HOST_NAME=`hostname -s`
echo "Hello from $USER_NAME at $HOST_NAME"

#make sure we are running the MPI version we want:
#which mpirun

# execute program here:
#  -np = number of cpus to use
#  --hostfile = name of hostfile to use, not required if MPI is compiled with torque (mpi-tor modules)

echo $temp > 'temp'
echo $fd20 > 'fd20'
echo $protname > protname.txt
echo "1" > rr
echo "1" >> rr
echo "3" > gg
echo "3" >> gg
trjconv -f $traj -s $top -o ${protname}.xtc -pbc cluster -b $SIMB -e $SIME < rr
trjconv -f ${protname}.xtc -s $top -o ${protname}.xtc -fit rot+trans < rr
trjconv -f ${protname}.xtc -s $top -o ${protname}.g96 < gg
trjconv -f ${protname}.xtc -o ${protname}_first.pdb -s $top -dump $SIMB < rr
trjconv -f ${protname}.xtc -o ${protname}.gro -s $top -dump $SIMB < rr
g_sas -f ${protname}.xtc -s $top -or resarea.xvg < rr
rm ./#*#
gfortran ../codes/input.f -o input.exe
./input.exe
gfortran ../codes/pdb_cmp.f03 -o pdb_cmp.exe
./pdb_cmp.exe
gfortran ../codes/resarea.f -o resarea.exe
./resarea.exe
gfortran ../codes/umatrix_cmp.f -o umatrix_cmp.exe
./umatrix_cmp.exe
echo "2.71828" > internalv
gfortran ../codes/LUI_calc_cmp.f03 -o LUI_calc_cmp.exe -llapack
./LUI_calc_cmp.exe
gfortran ../codes/mode_rect.f -o mode_rect.exe
./mode_rect.exe
gfortran ../codes/mode_mad.f -o mode_mad.exe
./mode_mad.exe
gfortran ../codes/indexNH.f -o indexNH.exe
./indexNH.exe
trjconv -f ${protname}.xtc -s $top -o nitro.g96 -n nitro.ndx
trjconv -f ${protname}.xtc -s $top -o hydro.g96 -n hydro.ndx
gfortran ../codes/NH_Qmatrix_cmp.f -o NH_Qmatrix_cmp.exe
./NH_Qmatrix_cmp.exe
gfortran ../codes/p2_gNH_intCA_cmp.f -o p2_gNH_intCA_cmp.exe
./p2_gNH_intCA.exe
echo "1.0" > NHfactor.dat
gfortran ../codes/T1T2_NH_cmp.f -o T1T2_NH_cmp.exe
./T1T2_NH_cmp.exe
gfortran ../codes/gather_cmp.f -o gather_cmp.exe
./gather_cmp.exe
